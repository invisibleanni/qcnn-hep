{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3.6 (tensorflow)","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"ResNet v1","provenance":[{"file_id":"https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_3_resnet.ipynb","timestamp":1623235796274}],"collapsed_sections":["zelI6ki5hKmy"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XESWfLvlZGHp"},"source":["# Google CoLab Instructions\n","\n","The following code ensures that Google CoLab is running the correct version of TensorFlow."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fU9UhAxTmp8S","executionInfo":{"status":"ok","timestamp":1623260523317,"user_tz":-420,"elapsed":7312,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"e9d668be-bce6-47c7-d829-9a88384f1371"},"source":["# Detect Colab if present\n","try:\n","    from google.colab import drive\n","    COLAB = True\n","    print(\"Note: using Google CoLab\")\n","    %tensorflow_version 2.x\n","except:\n","    print(\"Note: not using Google CoLab\")\n","    COLAB = False\n","\n","# Nicely formatted time string\n","def hms_string(sec_elapsed):\n","    h = int(sec_elapsed / (60 * 60))\n","    m = int((sec_elapsed % (60 * 60)) / 60)\n","    s = sec_elapsed % 60\n","    return f\"{h}:{m:>02}:{s:>05.2f}\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Note: using Google CoLab\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLFv_X4i3sVX","executionInfo":{"status":"ok","timestamp":1623260582012,"user_tz":-420,"elapsed":544,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"0c389d1e-4de5-4e5a-b248-4f6f98626c4b"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Wed Jun  9 17:43:01 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5XslUsqf3tSa","executionInfo":{"status":"ok","timestamp":1623260592994,"user_tz":-420,"elapsed":531,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"0c0ee9c6-0644-4e62-a891-5b2bbc925581"},"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Your runtime has 13.6 gigabytes of available RAM\n","\n","To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"\n","menu, and then select High-RAM in the Runtime shape dropdown. Then, \n","re-execute this cell.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q09yMGGcmp9N"},"source":["# Part 6.3: Implementing a ResNet in Keras\n","\n","Deeper neural networks are more difficult to train. Residual learning was introduced to ease the training of networks that are substantially deeper than those used previously. ResNet explicitly reformulates the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. On the ImageNet dataset this method was evaluated with residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. This technique can also be applied to the CIFAR-10 with 100 and 1000 layers. \n","\n","ResNet was introduced in the following paper:\n","\n","* K. He, X. Zhang, S. Ren, and J. Sun. [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385). arXiv preprint arXiv:1512.03385,2015.\n","\n","What is a residual?\n","\n","* [Residual](https://www.merriam-webster.com/dictionary/residual): an internal aftereffect of experience or activity that influences later behavior\n","\n","To implement a ResNet we need to give Keras the notion of a residual block.  This is essentially two dense layers with a \"skip connection\" (or residual connection).  A residual block is shown in Figure 6.SKIP.\n","\n","\n","**Figure 6.SKIP: Skip Layers**\n","![Skip Layers](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/skip-layer.png \"Skip Layers\")\n","\n","Residual blocks are typically used with convolutional neural networks (CNNs).  This allows very deep neural networks of CNNs to be created.  Figure 6.RES shows several different ResNets.\n","\n","**Figure 6.RES: ResNets**\n","![ResNets](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/resnet.png \"ResNets\")\n","\n","\n","### Keras Sequence vs Functional Model API\n","\n","Most of the neural networks create in this course have made use of the Keras sequence object.  You might have noticed that we briefly made use of another type of neural network object for the ResNet, the Model.  These are the [two major means](https://keras.io/getting-started/functional-api-guide/) of constructing a neural network in Keras:\n","\n","* [Sequential](https://keras.io/getting-started/sequential-model-guide/) - Simplified interface to Keras that supports most models where the flow of information is a simple sequence from input to output. \n","* [Keras Functional API](https://keras.io/getting-started/functional-api-guide/) - More complex interface that allows neural networks to be constructed of reused layers, multiple input layers, and supports building your own recurrent connections.\n","\n","It is important to point out that these are not two specific types of neural network.  Rather, they are two means of constructing neural networks in Keras.  Some types of neural network can be implemented in either, such as dense feedforward neural networks (like we used for the Iris and MPG datasets).  However, other types of neural network, like ResNet and GANs can only be used in the Functional Model API.\n","\n","### CIFAR Dataset\n","\n","The [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) datasets are also frequently used by the neural network research community.  These datasets were originally part of a competition. \n","\n","The CIFAR-10 data set contains low-res images that are divided into 10 classes.  The CIFAR-100 data set contains 100 classes in a hierarchy. \n"]},{"cell_type":"markdown","metadata":{"id":"zelI6ki5hKmy"},"source":["# Testing the CIFAR10"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"vsHA5OBsZ2A3","executionInfo":{"status":"error","timestamp":1623239913921,"user_tz":-420,"elapsed":3392,"user":{"displayName":"Eraraya Ricardo Muten","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggpw7xw-lyk6u6l92QjpI7MlI7qjJuuciCpwrUd=s64","userId":"03770692095188133952"}},"outputId":"2f66f2b2-213d-41a2-e629-6a80cd5a24ea"},"source":["# Load the CIFAR10 data.\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","print(x_train.shape, x_test.shape)\n","print(y_train.shape, y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n"," 15671296/170498071 [=>............................] - ETA: 22s"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-703ed42e57b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the CIFAR10 data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/cifar10.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0muntar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mfile_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       '6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce')\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0mnum_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"372-21BTmp9P"},"source":["Samples from the loaded CIFAR dataset can be displayed using the following code."]},{"cell_type":"code","metadata":{"id":"ZyJdYNTDmp9P"},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from six.moves import cPickle \n","\n","ROWS = 10\n","\n","x = x_train.astype(\"uint8\")\n","\n","fig, axes1 = plt.subplots(ROWS,ROWS,figsize=(10,10))\n","for j in range(ROWS):\n","    for k in range(ROWS):\n","        i = np.random.choice(range(len(x)))\n","        axes1[j][k].set_axis_off()\n","        axes1[j][k].imshow(x[i:i+1][0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OS1fOz1Jo9nl"},"source":["# Load Packages"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wAEaWreaD3c","executionInfo":{"status":"ok","timestamp":1623260625994,"user_tz":-420,"elapsed":19755,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"6ac4f221-a7ad-45b0-d593-181ac696deef"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gx-DMPysmp9N","executionInfo":{"status":"ok","timestamp":1623260628090,"user_tz":-420,"elapsed":2100,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}}},"source":["from __future__ import print_function\n","import tensorflow.keras\n","from tensorflow.keras.layers import Dense, Conv2D\n","from tensorflow.keras.layers import BatchNormalization, Activation\n","from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.datasets import cifar10\n","import numpy as np\n","import os\n","import h5py\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6xdVXeuBasG3"},"source":["# Checking the Dataset"]},{"cell_type":"markdown","metadata":{"id":"T-3SoTFADBYr"},"source":["py = 0, ey = 1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmMVdpFqZ8h-","executionInfo":{"status":"ok","timestamp":1623260700781,"user_tz":-420,"elapsed":7606,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"c11314a5-353a-41b9-8b91-66473d5c3142"},"source":["with h5py.File(\"/content/drive/My Drive/Projects/GSoC 2021/electron-photon-dataset/photon.hdf5\", \"r\") as f:\n","  px = np.asarray(f['X'][()], dtype=np.float32)\n","  py = np.asarray(f['y'][()], dtype=np.float32)\n","  print(\"Loaded photon dataset!\")\n","\n","with h5py.File(\"/content/drive/My Drive/Projects/GSoC 2021/electron-photon-dataset/electron.hdf5\", \"r\") as f:\n","  ex = np.asarray(f['X'][()], dtype=np.float32)\n","  ey = np.asarray(f['y'][()], dtype=np.float32)\n","  print(\"Loaded electron dataset!\")\n","\n","print(\"Photon dataset shape:\", px.shape, py.shape)\n","print(\"Electron dataset shape:\", ex.shape, ey.shape)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Loaded photon dataset!\n","Loaded electron dataset!\n","Photon dataset shape: (249000, 32, 32, 2) (249000,)\n","Electron dataset shape: (249000, 32, 32, 2) (249000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XP08ZPFOaY1O","executionInfo":{"status":"ok","timestamp":1623260701770,"user_tz":-420,"elapsed":993,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"17c9cea4-24f8-4c26-e7e1-16bccae37dbe"},"source":["max_photon, min_photon = np.max(px[:, :, :, 0]), np.min(px[:, :, :, 0])\n","max_electron, min_electron = np.max(ex[:, :, :, 0]), np.min(ex[:, :, :, 0])\n","\n","print(max_photon, min_photon)\n","print(max_electron, min_electron)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["1.4849443 0.0\n","1.431813 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68JbAEPpiJug","executionInfo":{"status":"ok","timestamp":1623260701770,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"530d8b55-db6d-43c1-d191-9d210658e15e"},"source":["max(max_photon, abs(min_photon), max_electron, abs(min_electron))"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.4849443"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FtO_4aJfZ7ue","executionInfo":{"status":"ok","timestamp":1623260709641,"user_tz":-420,"elapsed":3438,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"0e7b4bd6-9950-4120-a5fc-8f2efd89bef0"},"source":["X = np.concatenate((px[:, :, :, 0], ex[:, :, :, 0]), axis=0)\n","y = np.concatenate((py, ey), axis=0)\n","\n","x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2021)\n","\n","x_train = x_train.reshape(-1, 32, 32, 1)\n","x_test = x_test.reshape(-1, 32, 32, 1)\n","y_train = y_train.reshape(-1, 1)\n","y_test = y_test.reshape(-1, 1)\n","\n","print(X.shape, y.shape)\n","print(\"Train set shape:\", x_train.shape, y_train.shape)\n","print(\"Test set shape:\", x_test.shape, y_test.shape)\n","\n","del px, py, ex, ey"],"execution_count":16,"outputs":[{"output_type":"stream","text":["(498000, 32, 32) (498000,)\n","Train set shape: (398400, 32, 32, 1) (398400, 1)\n","Test set shape: (99600, 32, 32, 1) (99600, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3n8M3qWDPAi","executionInfo":{"status":"ok","timestamp":1623260711322,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"fea1749d-584a-46a0-c2f3-33c41ae70368"},"source":["X[y==0].shape"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(249000, 32, 32)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":852},"id":"8ZZveVrjAupF","executionInfo":{"status":"error","timestamp":1623260658628,"user_tz":-420,"elapsed":8159,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"32463132-dd57-4b83-d232-05d81e0d599c"},"source":["fig, ax = plt.subplots(32,32,figsize=(100, 121))\n","#fig.tight_layout(pad=1.7)\n","plt.subplots_adjust(bottom=0.05, right=0.98, left=0.1, top=0.1)\n","#plt.subplots_adjust(hspace=0.3)\n","for i in range(32):\n","  for j in range(32):\n","    ax[i, j].hist(X[y==0][:, i, j], alpha=0.5, color='b', label='photon')\n","    ax[i, j].hist(X[y==1][:, i, j], alpha=0.5, color='r', label='electron')\n","    ax[i, j].title.set_text(str(i) + \", \" + str(j))\n","    \n","fig.suptitle(\"Pixel Distribution\", fontweight=\"bold\")\n","plt.legend()\n","plt.show()"],"execution_count":11,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-ea9479ed21fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#fig.tight_layout(pad=1.7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.98\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#plt.subplots_adjust(hspace=0.3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n\u001b[1;32m   1179\u001b[0m                        \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_kw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m                        gridspec_kw=gridspec_kw)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m   1574\u001b[0m                 \u001b[0msubplot_kw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sharex\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_with\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msharex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m                 \u001b[0msubplot_kw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sharey\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_with\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msharey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m                 \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m         \u001b[0;31m# turn off redundant tick labeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36m_add_axes_internal\u001b[0;34m(self, key, ax)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0;34m\"\"\"Private helper for `add_axes` and `add_subplot`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_ax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msca\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m   1959\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[0;34m\"\"\"Set the current axes to be a and return a.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1961\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbubble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1962\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axobservers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mbubble\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbubble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entry_from_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mbubble\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mbubbles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthiso\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mthiso\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m                 \u001b[0mbubbles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthiso\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"stream","text":["Error in callback <function flush_figures at 0x7fd499546440> (for post_execute):\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# ignore the tracking, just draw and close all figures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# safely show traceback if in IPython, else raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                     bbox_inches = self.figure.get_tightbbox(renderer,\n\u001b[0;32m-> 2103\u001b[0;31m                             bbox_extra_artists=bbox_artists)\n\u001b[0m\u001b[1;32m   2104\u001b[0m                     \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pad_inches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m             \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m                 \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   4325\u001b[0m                 \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_xaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4327\u001b[0;31m             \u001b[0mbb_yaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbb_yaxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4329\u001b[0m                 \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_yaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;31m# go back to just this axis's tick labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2311\u001b[0m         \u001b[0;31m# get bounding boxes for this axis and any siblings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m         \u001b[0;31m# that have been set by `fig.align_ylabels()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2296\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m             \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2298\u001b[0;31m             \u001b[0mtlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2299\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m             \u001b[0mbboxes2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0;32m-> 1174\u001b[0;31m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0m\u001b[1;32m   1175\u001b[0m                 [tick.label2.get_window_extent(renderer)\n\u001b[1;32m   1176\u001b[0m                  for tick in ticks if tick.label2.get_visible()])\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0;32m-> 1174\u001b[0;31m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0m\u001b[1;32m   1175\u001b[0m                 [tick.label2.get_window_extent(renderer)\n\u001b[1;32m   1176\u001b[0m                  for tick in ticks if tick.label2.get_visible()])\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdpi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0;31m# Transform the values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_non_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0;31m# Convert the result back to the shape of the input values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mtransform_affine\u001b[0;34m(self, points)\u001b[0m\n\u001b[1;32m   2363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m         \u001b[0;31m# docstring inherited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2365\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform_non_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mget_affine\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2392\u001b[0m             return Affine2D(np.dot(self._b.get_affine().get_matrix(),\n\u001b[0;32m-> 2393\u001b[0;31m                                 self._a.get_affine().get_matrix()))\n\u001b[0m\u001b[1;32m   2394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"AKQDnk2Gmp9R"},"source":["We will construct a ResNet and train it on the dataset.  The following block of code defines some constant values that define how the network is constructed."]},{"cell_type":"code","metadata":{"id":"E60qRcFzmp9R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623260718473,"user_tz":-420,"elapsed":504,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"dabecab4-7f4d-41ef-ad39-e91e7d8fd6cd"},"source":["# Training parameters\n","BATCH_SIZE = 128  # orig paper trained all networks with batch_size=128\n","EPOCHS = 200 # 200\n","USE_AUGMENTATION = False\n","NUM_CLASSES = np.unique(y_train).shape[0] # 10\n","COLORS = x_train.shape[3]\n","\n","# Subtracting pixel mean improves accuracy\n","SUBTRACT_PIXEL_MEAN = True\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), \n","# Improved ResNet: version = 2 \n","# (ResNet v2)\n","VERSION = 1\n","\n","# Computed depth from supplied model parameter n\n","if VERSION == 1:\n","    DEPTH = COLORS * 6 + 2\n","elif VERSION == 2:\n","    DEPTH = COLORS * 9 + 2\n","\n","# Print parameters for sanity check\n","print(\"Batch size, epochs:\", BATCH_SIZE, EPOCHS)\n","print(\"Total class:\", NUM_CLASSES)\n","print(\"Use Augmentation:\", USE_AUGMENTATION)\n","print(\"Subtracting pixel mean:\", SUBTRACT_PIXEL_MEAN)\n","print(\"Color channel:\", COLORS)\n","print(\"ResNet Version:\", VERSION)\n","print(\"Network Depth:\", DEPTH)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Batch size, epochs: 128 200\n","Total class: 2\n","Use Augmentation: False\n","Subtracting pixel mean: True\n","Color channel: 1\n","ResNet Version: 1\n","Network Depth: 8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I91EXi2hmp9T"},"source":["The following function implements a learning rate decay schedule."]},{"cell_type":"code","metadata":{"id":"pHUm2jSUmp9T","executionInfo":{"status":"ok","timestamp":1623260724768,"user_tz":-420,"elapsed":481,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}}},"source":["def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7-du9R2rmp9V"},"source":["The following code implements a ResNet block.  This includes two convolutional layers with a skip connection.  Both V1 and V2 of ResNet make use of this type of layer."]},{"cell_type":"code","metadata":{"id":"DPXtreZ6mp9V","executionInfo":{"status":"ok","timestamp":1623260725275,"user_tz":-420,"elapsed":3,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}}},"source":["def resnet_layer(inputs,\n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n","\n","    # Arguments\n","        inputs (tensor): input tensor from input image or previous layer\n","        num_filters (int): Conv2D number of filters\n","        kernel_size (int): Conv2D square kernel dimensions\n","        strides (int): Conv2D square stride dimensions\n","        activation (string): activation name\n","        batch_normalization (bool): whether to include batch normalization\n","        conv_first (bool): conv-bn-activation (True) or\n","            bn-activation-conv (False)\n","\n","    # Returns\n","        x (tensor): tensor as input to the next layer\n","    \"\"\"\n","    conv = Conv2D(num_filters,\n","                  kernel_size=kernel_size,\n","                  strides=strides,\n","                  padding='same',\n","                  kernel_initializer='he_normal',\n","                  kernel_regularizer=l2(1e-4))\n","\n","    x = inputs\n","    if conv_first:\n","        x = conv(x)\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","    else:\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","        x = conv(x)\n","    return x"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FDk1ekrxmp9X"},"source":["### ResNet V1\n","\n","* K. He, X. Zhang, S. Ren, and J. Sun. [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385). arXiv preprint arXiv:1512.03385,2015."]},{"cell_type":"code","metadata":{"id":"YLQe8BE-mp9X","executionInfo":{"status":"ok","timestamp":1623260727013,"user_tz":-420,"elapsed":3,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}}},"source":["def resnet_v1(input_shape, depth, num_classes=10):\n","    \"\"\"ResNet Version 1 Model builder [a]\n","\n","    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n","    Last ReLU is after the shortcut connection.\n","    At the beginning of each stage, the feature \n","    map size is halved (downsampled)\n","    by a convolutional layer with strides=2, while the number of \n","    filters is\n","    doubled. Within each stage, the layers have the same number \n","    filters and the same number of filters.\n","    Features maps sizes:\n","    stage 0: 32x32, 16\n","    stage 1: 16x16, 32\n","    stage 2:  8x8,  64\n","    The Number of parameters is approx the same as Table 6 of [a]:\n","    ResNet20 0.27M\n","    ResNet32 0.46M\n","    ResNet44 0.66M\n","    ResNet56 0.85M\n","    ResNet110 1.7M\n","\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 6 != 0:\n","        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n","    # Start model definition.\n","    num_filters = 16\n","    num_res_blocks = int((depth - 2) / 6)\n","\n","    inputs = Input(shape=input_shape)\n","    x = resnet_layer(inputs=inputs)\n","    # Instantiate the stack of residual units\n","    for stack in range(3):\n","        for res_block in range(num_res_blocks):\n","            strides = 1\n","            # first layer but not first stack\n","            if stack > 0 and res_block == 0:  \n","                strides = 2  # downsample\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters,\n","                             strides=strides)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters,\n","                             activation=None)\n","            # first layer but not first stack\n","            if stack > 0 and res_block == 0:  \n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = tensorflow.keras.layers.add([x, y])\n","            x = Activation('relu')(x)\n","        num_filters *= 2\n","\n","    # Add classifier on top.\n","    # v1 does not use BN after last shortcut connection-ReLU\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SzNJPXncmp9a"},"source":["### ResNet V2\n","\n","A second version of ResNet was introduced in the following paper.  This form of ResNet is commonly referred to as ResNet V2.\n","\n","* He, K., Zhang, X., Ren, S., & Sun, J. (2016, October). [Identity mappings in deep residual networks](https://arxiv.org/abs/1603.05027). In European conference on computer vision (pp. 630-645). Springer, Cham.\n","\n","The following code constructs a ResNet V2 network.  The primary difference of the full preactivation 'v2' variant compared to the 'v1' variant is the use of [batch normalization](https://arxiv.org/abs/1502.03167) before every weight layer."]},{"cell_type":"code","metadata":{"id":"8WfihOPhmp9a","executionInfo":{"status":"ok","timestamp":1623260728898,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}}},"source":["def resnet_v2(input_shape, depth, num_classes=10):\n","    \"\"\"ResNet Version 2 Model builder [b]\n","\n","    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n","    bottleneck layer\n","    First shortcut connection per layer is 1 x 1 Conv2D.\n","    Second and onwards shortcut connection is identity.\n","    At the beginning of each stage, the feature map size is \n","    halved (downsampled) by a convolutional layer with \n","    strides=2, while the number of filter maps is\n","    doubled. Within each stage, the layers have the same\n","    number filters and the same filter map sizes.\n","    Features maps sizes:\n","    conv1  : 32x32,  16\n","    stage 0: 32x32,  64\n","    stage 1: 16x16, 128\n","    stage 2:  8x8,  256\n","\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 9 != 0:\n","        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n","    # Start model definition.\n","    num_filters_in = 16\n","    num_res_blocks = int((depth - 2) / 9)\n","\n","    inputs = Input(shape=input_shape)\n","    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n","    x = resnet_layer(inputs=inputs,\n","                     num_filters=num_filters_in,\n","                     conv_first=True)\n","\n","    # Instantiate the stack of residual units\n","    for stage in range(3):\n","        for res_block in range(num_res_blocks):\n","            activation = 'relu'\n","            batch_normalization = True\n","            strides = 1\n","            if stage == 0:\n","                num_filters_out = num_filters_in * 4\n","                if res_block == 0:  # first layer and first stage\n","                    activation = None\n","                    batch_normalization = False\n","            else:\n","                num_filters_out = num_filters_in * 2\n","                if res_block == 0:  # first layer but not first stage\n","                    strides = 2    # downsample\n","\n","            # bottleneck residual unit\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters_in,\n","                             kernel_size=1,\n","                             strides=strides,\n","                             activation=activation,\n","                             batch_normalization=batch_normalization,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_in,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_out,\n","                             kernel_size=1,\n","                             conv_first=False)\n","            if res_block == 0:\n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters_out,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = tensorflow.keras.layers.add([x, y])\n","\n","        num_filters_in = num_filters_out\n","\n","    # Add classifier on top.\n","    # v2 has BN-ReLU before Pooling\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-CzYSlxSmp9c"},"source":["With all of this defined, we can run the ResNet."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jg7cCBZnmp9c","executionInfo":{"status":"ok","timestamp":1623260748888,"user_tz":-420,"elapsed":5250,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"17fb35ce-7fcb-499f-f2f9-2672c69947b3"},"source":["# Input image dimensions.\n","input_shape = x_train.shape[1:]\n","pixel_max_val = max(max_photon, abs(min_photon), max_electron, abs(min_electron))\n","\n","# Normalize data.\n","x_train = x_train.astype('float32') / pixel_max_val\n","x_test = x_test.astype('float32') / pixel_max_val\n","\n","# If subtract pixel mean is enabled\n","if SUBTRACT_PIXEL_MEAN:\n","    x_train_mean = np.mean(x_train, axis=0)\n","    x_train -= x_train_mean\n","    x_test -= x_train_mean\n","\n","# Convert class vectors to binary class matrices.\n","y_train = tensorflow.keras.utils.to_categorical(y_train, NUM_CLASSES)\n","y_test = tensorflow.keras.utils.to_categorical(y_test, NUM_CLASSES)\n","\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","print('y_train shape:', y_train.shape)\n","\n","# Create the neural network\n","if VERSION == 2:\n","    model = resnet_v2(input_shape=input_shape, depth=DEPTH, num_classes=NUM_CLASSES)\n","else:\n","    model = resnet_v1(input_shape=input_shape, depth=DEPTH, num_classes=NUM_CLASSES)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(lr=lr_schedule(0)),\n","              metrics=['accuracy', tf.keras.metrics.AUC()])\n","model.summary()\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["x_train shape: (398400, 32, 32, 1)\n","398400 train samples\n","99600 test samples\n","y_train shape: (398400, 2)\n","Learning rate:  0.001\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 32, 32, 1)]  0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 32, 32, 16)   160         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n","                                                                 batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 16, 16, 32)   4640        activation_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 16, 16, 32)   128         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 16, 16, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 16, 16, 32)   9248        activation_3[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 16, 16, 32)   544         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 16, 16, 32)   128         conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 16, 16, 32)   0           conv2d_5[0][0]                   \n","                                                                 batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 16, 16, 32)   0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 8, 8, 64)     18496       activation_4[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 8, 8, 64)     256         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 8, 8, 64)     0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 8, 8, 64)     36928       activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 8, 8, 64)     2112        activation_4[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 8, 8, 64)     256         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 8, 8, 64)     0           conv2d_8[0][0]                   \n","                                                                 batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 8, 8, 64)     0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 2)            130         flatten[0][0]                    \n","==================================================================================================\n","Total params: 77,858\n","Trainable params: 77,378\n","Non-trainable params: 480\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CC1x1NIvmp9f","executionInfo":{"status":"ok","timestamp":1623260797466,"user_tz":-420,"elapsed":526,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}}},"source":["# Prepare callbacks for model saving and for learning rate adjustment.\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","\n","cp_filepath = \"/content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v\" + str(VERSION) + \"-{epoch:02d}epoch.h5\"\n","checkpoint_callback_epoch = tf.keras.callbacks.ModelCheckpoint(\n","    cp_filepath, monitor='val_loss', verbose=1, save_best_only=False,\n","    save_weights_only=False, mode='auto', save_freq='epoch'\n",")\n","# cp_filepath_batch = \"./Model/ResNet-v\" + str(VERSION) + \"-{epoch:02d}epoch-{batch:02d}batch.h5\"\n","# checkpoint_callback_batch = tf.keras.callbacks.ModelCheckpoint(\n","#     cp_filepath_batch, monitor='val_loss', verbose=1, save_best_only=False,\n","#     save_weights_only=False, mode='auto', save_freq=500\n","# )\n","\n","callbacks = [lr_reducer, lr_scheduler, checkpoint_callback_epoch]"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EIHcWL5Qd1k","executionInfo":{"status":"ok","timestamp":1623279304485,"user_tz":-420,"elapsed":1426989,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"6b0a443f-a033-45a9-9a1e-2db605074df9"},"source":["import time\n","start_time = time.time()\n","\n","last_epoch = 200\n","EPOCHS = 400\n","\n","# Run training, with or without data augmentation.\n","if not USE_AUGMENTATION:\n","    print('Not using data augmentation.')\n","    H = model.fit(x_train, y_train, initial_epoch=last_epoch,\n","              batch_size=BATCH_SIZE,\n","              epochs=EPOCHS,\n","              validation_data=(x_test, y_test),\n","              shuffle=True,\n","              callbacks=callbacks)\n","else:\n","    print('Using real-time data augmentation.')\n","    # This will do preprocessing and realtime data augmentation:\n","    datagen = ImageDataGenerator(\n","        # set input mean to 0 over the dataset\n","        featurewise_center=False,\n","        # set each sample mean to 0\n","        samplewise_center=False,\n","        # divide inputs by std of dataset\n","        featurewise_std_normalization=False,\n","        # divide each input by its std\n","        samplewise_std_normalization=False,\n","        # apply ZCA whitening\n","        zca_whitening=False,\n","        # epsilon for ZCA whitening\n","        zca_epsilon=1e-06,\n","        # randomly rotate images in the range (deg 0 to 180)\n","        rotation_range=0,\n","        # randomly shift images horizontally\n","        width_shift_range=0.1,\n","        # randomly shift images vertically\n","        height_shift_range=0.1,\n","        # set range for random shear\n","        shear_range=0.,\n","        # set range for random zoom\n","        zoom_range=0.,\n","        # set range for random channel shifts\n","        channel_shift_range=0.,\n","        # set mode for filling points outside the input boundaries\n","        fill_mode='nearest',\n","        # value used for fill_mode = \"constant\"\n","        cval=0.,\n","        # randomly flip images\n","        horizontal_flip=True,\n","        # randomly flip images\n","        vertical_flip=False,\n","        # set rescaling factor (applied before any other transformation)\n","        rescale=None,\n","        # set function that will be applied on each input\n","        preprocessing_function=None,\n","        # image data format, either \"channels_first\" or \"channels_last\"\n","        data_format=None,\n","        # fraction of images reserved for validation \n","        # (strictly between 0 and 1)\n","        validation_split=0.0)\n","\n","    # Compute quantities required for featurewise normalization\n","    # (std, mean, and principal components if ZCA whitening is applied).\n","    datagen.fit(x_train)\n","\n","    # Fit the model on the batches generated by datagen.flow().\n","    H = model.fit_generator(datagen.flow(x_train, y_train, \n","                        batch_size=BATCH_SIZE),\n","                        validation_data=(x_test, y_test),\n","                        epochs=EPOCHS, verbose=1, workers=1,\n","                        callbacks=callbacks, \n","                        use_multiprocessing=False)\n","    \n","elapsed_time = time.time() - start_time\n","print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Not using data augmentation.\n","Epoch 201/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5182 - accuracy: 0.7530 - auc: 0.8277 - val_loss: 0.5416 - val_accuracy: 0.7374 - val_auc: 0.8088\n","\n","Epoch 00201: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-201epoch.h5\n","Epoch 202/400\n","Learning rate:  5e-07\n","   6/3113 [..............................] - ETA: 34s - loss: 0.5210 - accuracy: 0.7474 - auc: 0.8277"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5429 - val_accuracy: 0.7360 - val_auc: 0.8075\n","\n","Epoch 00202: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-202epoch.h5\n","Epoch 203/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7530 - auc: 0.8278 - val_loss: 0.5421 - val_accuracy: 0.7366 - val_auc: 0.8082\n","\n","Epoch 00203: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-203epoch.h5\n","Epoch 204/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7539 - auc: 0.8280 - val_loss: 0.5425 - val_accuracy: 0.7363 - val_auc: 0.8078\n","\n","Epoch 00204: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-204epoch.h5\n","Epoch 205/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5421 - val_accuracy: 0.7368 - val_auc: 0.8084\n","\n","Epoch 00205: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-205epoch.h5\n","Epoch 206/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5422 - val_accuracy: 0.7368 - val_auc: 0.8082\n","\n","Epoch 00206: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-206epoch.h5\n","Epoch 207/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5419 - val_accuracy: 0.7370 - val_auc: 0.8085\n","\n","Epoch 00207: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-207epoch.h5\n","Epoch 208/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5417 - val_accuracy: 0.7372 - val_auc: 0.8086\n","\n","Epoch 00208: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-208epoch.h5\n","Epoch 209/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7535 - auc: 0.8278 - val_loss: 0.5411 - val_accuracy: 0.7375 - val_auc: 0.8092\n","\n","Epoch 00209: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-209epoch.h5\n","Epoch 210/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5412 - val_accuracy: 0.7373 - val_auc: 0.8092\n","\n","Epoch 00210: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-210epoch.h5\n","Epoch 211/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7533 - auc: 0.8278 - val_loss: 0.5430 - val_accuracy: 0.7360 - val_auc: 0.8075\n","\n","Epoch 00211: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-211epoch.h5\n","Epoch 212/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5425 - val_accuracy: 0.7366 - val_auc: 0.8079\n","\n","Epoch 00212: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-212epoch.h5\n","Epoch 213/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5425 - val_accuracy: 0.7365 - val_auc: 0.8079\n","\n","Epoch 00213: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-213epoch.h5\n","Epoch 214/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7537 - auc: 0.8279 - val_loss: 0.5429 - val_accuracy: 0.7362 - val_auc: 0.8076\n","\n","Epoch 00214: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-214epoch.h5\n","Epoch 215/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7532 - auc: 0.8278 - val_loss: 0.5424 - val_accuracy: 0.7366 - val_auc: 0.8079\n","\n","Epoch 00215: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-215epoch.h5\n","Epoch 216/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7531 - auc: 0.8278 - val_loss: 0.5429 - val_accuracy: 0.7361 - val_auc: 0.8074\n","\n","Epoch 00216: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-216epoch.h5\n","Epoch 217/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7530 - auc: 0.8279 - val_loss: 0.5425 - val_accuracy: 0.7365 - val_auc: 0.8078\n","\n","Epoch 00217: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-217epoch.h5\n","Epoch 218/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5419 - val_accuracy: 0.7369 - val_auc: 0.8085\n","\n","Epoch 00218: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-218epoch.h5\n","Epoch 219/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7531 - auc: 0.8279 - val_loss: 0.5415 - val_accuracy: 0.7373 - val_auc: 0.8088\n","\n","Epoch 00219: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-219epoch.h5\n","Epoch 220/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5427 - val_accuracy: 0.7361 - val_auc: 0.8077\n","\n","Epoch 00220: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-220epoch.h5\n","Epoch 221/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7536 - auc: 0.8280 - val_loss: 0.5413 - val_accuracy: 0.7373 - val_auc: 0.8090\n","\n","Epoch 00221: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-221epoch.h5\n","Epoch 222/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5423 - val_accuracy: 0.7369 - val_auc: 0.8081\n","\n","Epoch 00222: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-222epoch.h5\n","Epoch 223/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7537 - auc: 0.8280 - val_loss: 0.5417 - val_accuracy: 0.7372 - val_auc: 0.8086\n","\n","Epoch 00223: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-223epoch.h5\n","Epoch 224/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5421 - val_accuracy: 0.7368 - val_auc: 0.8082\n","\n","Epoch 00224: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-224epoch.h5\n","Epoch 225/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5421 - val_accuracy: 0.7367 - val_auc: 0.8082\n","\n","Epoch 00225: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-225epoch.h5\n","Epoch 226/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5177 - accuracy: 0.7536 - auc: 0.8281 - val_loss: 0.5423 - val_accuracy: 0.7366 - val_auc: 0.8081\n","\n","Epoch 00226: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-226epoch.h5\n","Epoch 227/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7535 - auc: 0.8278 - val_loss: 0.5435 - val_accuracy: 0.7356 - val_auc: 0.8071\n","\n","Epoch 00227: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-227epoch.h5\n","Epoch 228/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5413 - val_accuracy: 0.7374 - val_auc: 0.8091\n","\n","Epoch 00228: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-228epoch.h5\n","Epoch 229/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7531 - auc: 0.8278 - val_loss: 0.5423 - val_accuracy: 0.7366 - val_auc: 0.8081\n","\n","Epoch 00229: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-229epoch.h5\n","Epoch 230/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5429 - val_accuracy: 0.7360 - val_auc: 0.8074\n","\n","Epoch 00230: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-230epoch.h5\n","Epoch 231/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5422 - val_accuracy: 0.7369 - val_auc: 0.8082\n","\n","Epoch 00231: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-231epoch.h5\n","Epoch 232/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5414 - val_accuracy: 0.7370 - val_auc: 0.8089\n","\n","Epoch 00232: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-232epoch.h5\n","Epoch 233/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7532 - auc: 0.8279 - val_loss: 0.5422 - val_accuracy: 0.7368 - val_auc: 0.8081\n","\n","Epoch 00233: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-233epoch.h5\n","Epoch 234/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7531 - auc: 0.8279 - val_loss: 0.5432 - val_accuracy: 0.7357 - val_auc: 0.8073\n","\n","Epoch 00234: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-234epoch.h5\n","Epoch 235/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5422 - val_accuracy: 0.7366 - val_auc: 0.8082\n","\n","Epoch 00235: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-235epoch.h5\n","Epoch 236/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5417 - val_accuracy: 0.7372 - val_auc: 0.8087\n","\n","Epoch 00236: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-236epoch.h5\n","Epoch 237/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7532 - auc: 0.8279 - val_loss: 0.5430 - val_accuracy: 0.7359 - val_auc: 0.8074\n","\n","Epoch 00237: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-237epoch.h5\n","Epoch 238/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7530 - auc: 0.8278 - val_loss: 0.5417 - val_accuracy: 0.7369 - val_auc: 0.8086\n","\n","Epoch 00238: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-238epoch.h5\n","Epoch 239/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7533 - auc: 0.8281 - val_loss: 0.5414 - val_accuracy: 0.7373 - val_auc: 0.8089\n","\n","Epoch 00239: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-239epoch.h5\n","Epoch 240/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5422 - val_accuracy: 0.7368 - val_auc: 0.8082\n","\n","Epoch 00240: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-240epoch.h5\n","Epoch 241/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7534 - auc: 0.8278 - val_loss: 0.5417 - val_accuracy: 0.7372 - val_auc: 0.8088\n","\n","Epoch 00241: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-241epoch.h5\n","Epoch 242/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7529 - auc: 0.8279 - val_loss: 0.5425 - val_accuracy: 0.7365 - val_auc: 0.8078\n","\n","Epoch 00242: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-242epoch.h5\n","Epoch 243/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5177 - accuracy: 0.7537 - auc: 0.8281 - val_loss: 0.5425 - val_accuracy: 0.7363 - val_auc: 0.8079\n","\n","Epoch 00243: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-243epoch.h5\n","Epoch 244/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7530 - auc: 0.8279 - val_loss: 0.5427 - val_accuracy: 0.7367 - val_auc: 0.8078\n","\n","Epoch 00244: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-244epoch.h5\n","Epoch 245/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5421 - val_accuracy: 0.7369 - val_auc: 0.8084\n","\n","Epoch 00245: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-245epoch.h5\n","Epoch 246/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5408 - val_accuracy: 0.7375 - val_auc: 0.8094\n","\n","Epoch 00246: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-246epoch.h5\n","Epoch 247/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5422 - val_accuracy: 0.7368 - val_auc: 0.8082\n","\n","Epoch 00247: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-247epoch.h5\n","Epoch 248/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7536 - auc: 0.8279 - val_loss: 0.5411 - val_accuracy: 0.7376 - val_auc: 0.8092\n","\n","Epoch 00248: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-248epoch.h5\n","Epoch 249/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5413 - val_accuracy: 0.7374 - val_auc: 0.8090\n","\n","Epoch 00249: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-249epoch.h5\n","Epoch 250/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7537 - auc: 0.8280 - val_loss: 0.5424 - val_accuracy: 0.7366 - val_auc: 0.8079\n","\n","Epoch 00250: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-250epoch.h5\n","Epoch 251/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7536 - auc: 0.8281 - val_loss: 0.5417 - val_accuracy: 0.7372 - val_auc: 0.8087\n","\n","Epoch 00251: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-251epoch.h5\n","Epoch 252/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7531 - auc: 0.8279 - val_loss: 0.5422 - val_accuracy: 0.7370 - val_auc: 0.8081\n","\n","Epoch 00252: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-252epoch.h5\n","Epoch 253/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5418 - val_accuracy: 0.7371 - val_auc: 0.8085\n","\n","Epoch 00253: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-253epoch.h5\n","Epoch 254/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7538 - auc: 0.8280 - val_loss: 0.5426 - val_accuracy: 0.7363 - val_auc: 0.8077\n","\n","Epoch 00254: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-254epoch.h5\n","Epoch 255/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5423 - val_accuracy: 0.7367 - val_auc: 0.8080\n","\n","Epoch 00255: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-255epoch.h5\n","Epoch 256/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7532 - auc: 0.8278 - val_loss: 0.5424 - val_accuracy: 0.7366 - val_auc: 0.8080\n","\n","Epoch 00256: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-256epoch.h5\n","Epoch 257/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5412 - val_accuracy: 0.7377 - val_auc: 0.8092\n","\n","Epoch 00257: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-257epoch.h5\n","Epoch 258/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7536 - auc: 0.8280 - val_loss: 0.5414 - val_accuracy: 0.7371 - val_auc: 0.8089\n","\n","Epoch 00258: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-258epoch.h5\n","Epoch 259/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5417 - val_accuracy: 0.7372 - val_auc: 0.8086\n","\n","Epoch 00259: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-259epoch.h5\n","Epoch 260/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8280 - val_loss: 0.5447 - val_accuracy: 0.7348 - val_auc: 0.8059\n","\n","Epoch 00260: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-260epoch.h5\n","Epoch 261/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5177 - accuracy: 0.7536 - auc: 0.8281 - val_loss: 0.5419 - val_accuracy: 0.7369 - val_auc: 0.8083\n","\n","Epoch 00261: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-261epoch.h5\n","Epoch 262/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5424 - val_accuracy: 0.7368 - val_auc: 0.8079\n","\n","Epoch 00262: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-262epoch.h5\n","Epoch 263/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5418 - val_accuracy: 0.7372 - val_auc: 0.8085\n","\n","Epoch 00263: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-263epoch.h5\n","Epoch 264/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5435 - val_accuracy: 0.7354 - val_auc: 0.8071\n","\n","Epoch 00264: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-264epoch.h5\n","Epoch 265/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8280 - val_loss: 0.5425 - val_accuracy: 0.7364 - val_auc: 0.8078\n","\n","Epoch 00265: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-265epoch.h5\n","Epoch 266/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7532 - auc: 0.8279 - val_loss: 0.5418 - val_accuracy: 0.7371 - val_auc: 0.8087\n","\n","Epoch 00266: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-266epoch.h5\n","Epoch 267/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7536 - auc: 0.8280 - val_loss: 0.5421 - val_accuracy: 0.7367 - val_auc: 0.8082\n","\n","Epoch 00267: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-267epoch.h5\n","Epoch 268/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5417 - val_accuracy: 0.7372 - val_auc: 0.8088\n","\n","Epoch 00268: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-268epoch.h5\n","Epoch 269/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7533 - auc: 0.8278 - val_loss: 0.5410 - val_accuracy: 0.7375 - val_auc: 0.8093\n","\n","Epoch 00269: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-269epoch.h5\n","Epoch 270/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7537 - auc: 0.8281 - val_loss: 0.5421 - val_accuracy: 0.7368 - val_auc: 0.8083\n","\n","Epoch 00270: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-270epoch.h5\n","Epoch 271/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7529 - auc: 0.8278 - val_loss: 0.5434 - val_accuracy: 0.7356 - val_auc: 0.8070\n","\n","Epoch 00271: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-271epoch.h5\n","Epoch 272/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7536 - auc: 0.8279 - val_loss: 0.5416 - val_accuracy: 0.7370 - val_auc: 0.8087\n","\n","Epoch 00272: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-272epoch.h5\n","Epoch 273/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7531 - auc: 0.8279 - val_loss: 0.5427 - val_accuracy: 0.7364 - val_auc: 0.8077\n","\n","Epoch 00273: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-273epoch.h5\n","Epoch 274/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7531 - auc: 0.8279 - val_loss: 0.5416 - val_accuracy: 0.7374 - val_auc: 0.8088\n","\n","Epoch 00274: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-274epoch.h5\n","Epoch 275/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7532 - auc: 0.8279 - val_loss: 0.5413 - val_accuracy: 0.7373 - val_auc: 0.8091\n","\n","Epoch 00275: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-275epoch.h5\n","Epoch 276/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5412 - val_accuracy: 0.7373 - val_auc: 0.8090\n","\n","Epoch 00276: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-276epoch.h5\n","Epoch 277/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7536 - auc: 0.8280 - val_loss: 0.5426 - val_accuracy: 0.7362 - val_auc: 0.8078\n","\n","Epoch 00277: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-277epoch.h5\n","Epoch 278/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8281 - val_loss: 0.5424 - val_accuracy: 0.7368 - val_auc: 0.8080\n","\n","Epoch 00278: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-278epoch.h5\n","Epoch 279/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7532 - auc: 0.8279 - val_loss: 0.5422 - val_accuracy: 0.7368 - val_auc: 0.8082\n","\n","Epoch 00279: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-279epoch.h5\n","Epoch 280/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5426 - val_accuracy: 0.7363 - val_auc: 0.8078\n","\n","Epoch 00280: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-280epoch.h5\n","Epoch 281/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5414 - val_accuracy: 0.7372 - val_auc: 0.8089\n","\n","Epoch 00281: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-281epoch.h5\n","Epoch 282/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5180 - accuracy: 0.7537 - auc: 0.8279 - val_loss: 0.5425 - val_accuracy: 0.7366 - val_auc: 0.8079\n","\n","Epoch 00282: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-282epoch.h5\n","Epoch 283/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5413 - val_accuracy: 0.7372 - val_auc: 0.8089\n","\n","Epoch 00283: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-283epoch.h5\n","Epoch 284/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7531 - auc: 0.8279 - val_loss: 0.5423 - val_accuracy: 0.7369 - val_auc: 0.8082\n","\n","Epoch 00284: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-284epoch.h5\n","Epoch 285/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5416 - val_accuracy: 0.7374 - val_auc: 0.8088\n","\n","Epoch 00285: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-285epoch.h5\n","Epoch 286/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5411 - val_accuracy: 0.7375 - val_auc: 0.8092\n","\n","Epoch 00286: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-286epoch.h5\n","Epoch 287/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7533 - auc: 0.8281 - val_loss: 0.5410 - val_accuracy: 0.7375 - val_auc: 0.8093\n","\n","Epoch 00287: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-287epoch.h5\n","Epoch 288/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5181 - accuracy: 0.7536 - auc: 0.8278 - val_loss: 0.5413 - val_accuracy: 0.7374 - val_auc: 0.8090\n","\n","Epoch 00288: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-288epoch.h5\n","Epoch 289/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5425 - val_accuracy: 0.7364 - val_auc: 0.8078\n","\n","Epoch 00289: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-289epoch.h5\n","Epoch 290/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5177 - accuracy: 0.7533 - auc: 0.8281 - val_loss: 0.5416 - val_accuracy: 0.7373 - val_auc: 0.8087\n","\n","Epoch 00290: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-290epoch.h5\n","Epoch 291/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 40s 13ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5413 - val_accuracy: 0.7373 - val_auc: 0.8090\n","\n","Epoch 00291: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-291epoch.h5\n","Epoch 292/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5417 - val_accuracy: 0.7369 - val_auc: 0.8086\n","\n","Epoch 00292: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-292epoch.h5\n","Epoch 293/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5180 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5421 - val_accuracy: 0.7366 - val_auc: 0.8082\n","\n","Epoch 00293: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-293epoch.h5\n","Epoch 294/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5178 - accuracy: 0.7537 - auc: 0.8280 - val_loss: 0.5410 - val_accuracy: 0.7374 - val_auc: 0.8092\n","\n","Epoch 00294: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-294epoch.h5\n","Epoch 295/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5178 - accuracy: 0.7537 - auc: 0.8280 - val_loss: 0.5415 - val_accuracy: 0.7372 - val_auc: 0.8089\n","\n","Epoch 00295: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-295epoch.h5\n","Epoch 296/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5177 - accuracy: 0.7536 - auc: 0.8282 - val_loss: 0.5422 - val_accuracy: 0.7369 - val_auc: 0.8081\n","\n","Epoch 00296: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-296epoch.h5\n","Epoch 297/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5178 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5423 - val_accuracy: 0.7369 - val_auc: 0.8081\n","\n","Epoch 00297: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-297epoch.h5\n","Epoch 298/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5180 - accuracy: 0.7530 - auc: 0.8279 - val_loss: 0.5424 - val_accuracy: 0.7367 - val_auc: 0.8079\n","\n","Epoch 00298: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-298epoch.h5\n","Epoch 299/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5176 - accuracy: 0.7534 - auc: 0.8282 - val_loss: 0.5419 - val_accuracy: 0.7369 - val_auc: 0.8084\n","\n","Epoch 00299: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-299epoch.h5\n","Epoch 300/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5180 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5413 - val_accuracy: 0.7373 - val_auc: 0.8090\n","\n","Epoch 00300: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-300epoch.h5\n","Epoch 301/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5179 - accuracy: 0.7536 - auc: 0.8280 - val_loss: 0.5408 - val_accuracy: 0.7376 - val_auc: 0.8096\n","\n","Epoch 00301: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-301epoch.h5\n","Epoch 302/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5178 - accuracy: 0.7536 - auc: 0.8280 - val_loss: 0.5420 - val_accuracy: 0.7370 - val_auc: 0.8084\n","\n","Epoch 00302: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-302epoch.h5\n","Epoch 303/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5414 - val_accuracy: 0.7371 - val_auc: 0.8088\n","\n","Epoch 00303: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-303epoch.h5\n","Epoch 304/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5180 - accuracy: 0.7532 - auc: 0.8279 - val_loss: 0.5414 - val_accuracy: 0.7371 - val_auc: 0.8089\n","\n","Epoch 00304: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-304epoch.h5\n","Epoch 305/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5427 - val_accuracy: 0.7361 - val_auc: 0.8077\n","\n","Epoch 00305: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-305epoch.h5\n","Epoch 306/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5180 - accuracy: 0.7530 - auc: 0.8278 - val_loss: 0.5416 - val_accuracy: 0.7373 - val_auc: 0.8088\n","\n","Epoch 00306: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-306epoch.h5\n","Epoch 307/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5177 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5425 - val_accuracy: 0.7369 - val_auc: 0.8079\n","\n","Epoch 00307: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-307epoch.h5\n","Epoch 308/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5415 - val_accuracy: 0.7372 - val_auc: 0.8088\n","\n","Epoch 00308: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-308epoch.h5\n","Epoch 309/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5178 - accuracy: 0.7531 - auc: 0.8280 - val_loss: 0.5418 - val_accuracy: 0.7370 - val_auc: 0.8085\n","\n","Epoch 00309: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-309epoch.h5\n","Epoch 310/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5422 - val_accuracy: 0.7367 - val_auc: 0.8082\n","\n","Epoch 00310: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-310epoch.h5\n","Epoch 311/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5176 - accuracy: 0.7534 - auc: 0.8282 - val_loss: 0.5420 - val_accuracy: 0.7369 - val_auc: 0.8085\n","\n","Epoch 00311: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-311epoch.h5\n","Epoch 312/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5424 - val_accuracy: 0.7370 - val_auc: 0.8080\n","\n","Epoch 00312: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-312epoch.h5\n","Epoch 313/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5178 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5409 - val_accuracy: 0.7376 - val_auc: 0.8094\n","\n","Epoch 00313: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-313epoch.h5\n","Epoch 314/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5177 - accuracy: 0.7535 - auc: 0.8281 - val_loss: 0.5416 - val_accuracy: 0.7373 - val_auc: 0.8087\n","\n","Epoch 00314: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-314epoch.h5\n","Epoch 315/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5177 - accuracy: 0.7534 - auc: 0.8281 - val_loss: 0.5426 - val_accuracy: 0.7365 - val_auc: 0.8078\n","\n","Epoch 00315: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-315epoch.h5\n","Epoch 316/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5417 - val_accuracy: 0.7369 - val_auc: 0.8087\n","\n","Epoch 00316: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-316epoch.h5\n","Epoch 317/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5178 - accuracy: 0.7533 - auc: 0.8281 - val_loss: 0.5439 - val_accuracy: 0.7349 - val_auc: 0.8066\n","\n","Epoch 00317: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-317epoch.h5\n","Epoch 318/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 41s 13ms/step - loss: 0.5180 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5411 - val_accuracy: 0.7375 - val_auc: 0.8092\n","\n","Epoch 00318: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-318epoch.h5\n","Epoch 319/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 44s 14ms/step - loss: 0.5177 - accuracy: 0.7532 - auc: 0.8281 - val_loss: 0.5420 - val_accuracy: 0.7371 - val_auc: 0.8083\n","\n","Epoch 00319: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-319epoch.h5\n","Epoch 320/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 44s 14ms/step - loss: 0.5177 - accuracy: 0.7533 - auc: 0.8281 - val_loss: 0.5420 - val_accuracy: 0.7370 - val_auc: 0.8085\n","\n","Epoch 00320: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-320epoch.h5\n","Epoch 321/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 44s 14ms/step - loss: 0.5179 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5432 - val_accuracy: 0.7357 - val_auc: 0.8073\n","\n","Epoch 00321: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-321epoch.h5\n","Epoch 322/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 44s 14ms/step - loss: 0.5178 - accuracy: 0.7533 - auc: 0.8280 - val_loss: 0.5434 - val_accuracy: 0.7357 - val_auc: 0.8071\n","\n","Epoch 00322: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-322epoch.h5\n","Epoch 323/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 44s 14ms/step - loss: 0.5177 - accuracy: 0.7535 - auc: 0.8281 - val_loss: 0.5412 - val_accuracy: 0.7373 - val_auc: 0.8090\n","\n","Epoch 00323: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-323epoch.h5\n","Epoch 324/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 44s 14ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5437 - val_accuracy: 0.7353 - val_auc: 0.8069\n","\n","Epoch 00324: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-324epoch.h5\n","Epoch 325/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7537 - auc: 0.8280 - val_loss: 0.5420 - val_accuracy: 0.7367 - val_auc: 0.8083\n","\n","Epoch 00325: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-325epoch.h5\n","Epoch 326/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5413 - val_accuracy: 0.7372 - val_auc: 0.8091\n","\n","Epoch 00326: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-326epoch.h5\n","Epoch 327/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5414 - val_accuracy: 0.7370 - val_auc: 0.8089\n","\n","Epoch 00327: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-327epoch.h5\n","Epoch 328/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5431 - val_accuracy: 0.7359 - val_auc: 0.8073\n","\n","Epoch 00328: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-328epoch.h5\n","Epoch 329/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5425 - val_accuracy: 0.7364 - val_auc: 0.8079\n","\n","Epoch 00329: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-329epoch.h5\n","Epoch 330/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5415 - val_accuracy: 0.7372 - val_auc: 0.8088\n","\n","Epoch 00330: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-330epoch.h5\n","Epoch 331/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5420 - val_accuracy: 0.7368 - val_auc: 0.8083\n","\n","Epoch 00331: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-331epoch.h5\n","Epoch 332/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5180 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5415 - val_accuracy: 0.7372 - val_auc: 0.8090\n","\n","Epoch 00332: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-332epoch.h5\n","Epoch 333/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5421 - val_accuracy: 0.7368 - val_auc: 0.8082\n","\n","Epoch 00333: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-333epoch.h5\n","Epoch 334/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5415 - val_accuracy: 0.7371 - val_auc: 0.8088\n","\n","Epoch 00334: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-334epoch.h5\n","Epoch 335/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5180 - accuracy: 0.7534 - auc: 0.8278 - val_loss: 0.5418 - val_accuracy: 0.7371 - val_auc: 0.8084\n","\n","Epoch 00335: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-335epoch.h5\n","Epoch 336/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5176 - accuracy: 0.7534 - auc: 0.8282 - val_loss: 0.5417 - val_accuracy: 0.7369 - val_auc: 0.8086\n","\n","Epoch 00336: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-336epoch.h5\n","Epoch 337/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5433 - val_accuracy: 0.7357 - val_auc: 0.8072\n","\n","Epoch 00337: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-337epoch.h5\n","Epoch 338/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5176 - accuracy: 0.7537 - auc: 0.8282 - val_loss: 0.5415 - val_accuracy: 0.7374 - val_auc: 0.8088\n","\n","Epoch 00338: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-338epoch.h5\n","Epoch 339/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5415 - val_accuracy: 0.7371 - val_auc: 0.8087\n","\n","Epoch 00339: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-339epoch.h5\n","Epoch 340/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7537 - auc: 0.8280 - val_loss: 0.5413 - val_accuracy: 0.7374 - val_auc: 0.8091\n","\n","Epoch 00340: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-340epoch.h5\n","Epoch 341/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5412 - val_accuracy: 0.7374 - val_auc: 0.8090\n","\n","Epoch 00341: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-341epoch.h5\n","Epoch 342/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5418 - val_accuracy: 0.7371 - val_auc: 0.8085\n","\n","Epoch 00342: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-342epoch.h5\n","Epoch 343/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5180 - accuracy: 0.7536 - auc: 0.8279 - val_loss: 0.5419 - val_accuracy: 0.7368 - val_auc: 0.8085\n","\n","Epoch 00343: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-343epoch.h5\n","Epoch 344/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5421 - val_accuracy: 0.7366 - val_auc: 0.8082\n","\n","Epoch 00344: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-344epoch.h5\n","Epoch 345/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7536 - auc: 0.8280 - val_loss: 0.5427 - val_accuracy: 0.7365 - val_auc: 0.8077\n","\n","Epoch 00345: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-345epoch.h5\n","Epoch 346/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5177 - accuracy: 0.7535 - auc: 0.8281 - val_loss: 0.5432 - val_accuracy: 0.7358 - val_auc: 0.8074\n","\n","Epoch 00346: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-346epoch.h5\n","Epoch 347/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5177 - accuracy: 0.7537 - auc: 0.8281 - val_loss: 0.5414 - val_accuracy: 0.7372 - val_auc: 0.8090\n","\n","Epoch 00347: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-347epoch.h5\n","Epoch 348/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5177 - accuracy: 0.7530 - auc: 0.8281 - val_loss: 0.5421 - val_accuracy: 0.7369 - val_auc: 0.8083\n","\n","Epoch 00348: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-348epoch.h5\n","Epoch 349/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5177 - accuracy: 0.7535 - auc: 0.8281 - val_loss: 0.5435 - val_accuracy: 0.7354 - val_auc: 0.8069\n","\n","Epoch 00349: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-349epoch.h5\n","Epoch 350/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5177 - accuracy: 0.7534 - auc: 0.8281 - val_loss: 0.5416 - val_accuracy: 0.7372 - val_auc: 0.8088\n","\n","Epoch 00350: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-350epoch.h5\n","Epoch 351/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5180 - accuracy: 0.7532 - auc: 0.8279 - val_loss: 0.5419 - val_accuracy: 0.7372 - val_auc: 0.8085\n","\n","Epoch 00351: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-351epoch.h5\n","Epoch 352/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5181 - accuracy: 0.7532 - auc: 0.8278 - val_loss: 0.5423 - val_accuracy: 0.7368 - val_auc: 0.8081\n","\n","Epoch 00352: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-352epoch.h5\n","Epoch 353/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5177 - accuracy: 0.7535 - auc: 0.8281 - val_loss: 0.5414 - val_accuracy: 0.7374 - val_auc: 0.8088\n","\n","Epoch 00353: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-353epoch.h5\n","Epoch 354/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5420 - val_accuracy: 0.7369 - val_auc: 0.8083\n","\n","Epoch 00354: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-354epoch.h5\n","Epoch 355/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5179 - accuracy: 0.7535 - auc: 0.8279 - val_loss: 0.5425 - val_accuracy: 0.7363 - val_auc: 0.8080\n","\n","Epoch 00355: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-355epoch.h5\n","Epoch 356/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5177 - accuracy: 0.7537 - auc: 0.8280 - val_loss: 0.5431 - val_accuracy: 0.7361 - val_auc: 0.8073\n","\n","Epoch 00356: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-356epoch.h5\n","Epoch 357/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5178 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5433 - val_accuracy: 0.7359 - val_auc: 0.8072\n","\n","Epoch 00357: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-357epoch.h5\n","Epoch 358/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5177 - accuracy: 0.7533 - auc: 0.8281 - val_loss: 0.5417 - val_accuracy: 0.7371 - val_auc: 0.8086\n","\n","Epoch 00358: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-358epoch.h5\n","Epoch 359/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 42s 13ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5423 - val_accuracy: 0.7366 - val_auc: 0.8079\n","\n","Epoch 00359: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-359epoch.h5\n","Epoch 360/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5176 - accuracy: 0.7537 - auc: 0.8282 - val_loss: 0.5413 - val_accuracy: 0.7375 - val_auc: 0.8090\n","\n","Epoch 00360: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-360epoch.h5\n","Epoch 361/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5423 - val_accuracy: 0.7366 - val_auc: 0.8081\n","\n","Epoch 00361: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-361epoch.h5\n","Epoch 362/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5180 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5424 - val_accuracy: 0.7370 - val_auc: 0.8080\n","\n","Epoch 00362: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-362epoch.h5\n","Epoch 363/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7533 - auc: 0.8280 - val_loss: 0.5426 - val_accuracy: 0.7362 - val_auc: 0.8077\n","\n","Epoch 00363: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-363epoch.h5\n","Epoch 364/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5179 - accuracy: 0.7532 - auc: 0.8279 - val_loss: 0.5423 - val_accuracy: 0.7369 - val_auc: 0.8081\n","\n","Epoch 00364: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-364epoch.h5\n","Epoch 365/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5176 - accuracy: 0.7535 - auc: 0.8281 - val_loss: 0.5425 - val_accuracy: 0.7368 - val_auc: 0.8080\n","\n","Epoch 00365: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-365epoch.h5\n","Epoch 366/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7533 - auc: 0.8281 - val_loss: 0.5419 - val_accuracy: 0.7367 - val_auc: 0.8084\n","\n","Epoch 00366: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-366epoch.h5\n","Epoch 367/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5177 - accuracy: 0.7536 - auc: 0.8281 - val_loss: 0.5414 - val_accuracy: 0.7372 - val_auc: 0.8089\n","\n","Epoch 00367: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-367epoch.h5\n","Epoch 368/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5412 - val_accuracy: 0.7374 - val_auc: 0.8092\n","\n","Epoch 00368: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-368epoch.h5\n","Epoch 369/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5177 - accuracy: 0.7537 - auc: 0.8280 - val_loss: 0.5420 - val_accuracy: 0.7367 - val_auc: 0.8083\n","\n","Epoch 00369: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-369epoch.h5\n","Epoch 370/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7531 - auc: 0.8281 - val_loss: 0.5420 - val_accuracy: 0.7369 - val_auc: 0.8085\n","\n","Epoch 00370: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-370epoch.h5\n","Epoch 371/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5179 - accuracy: 0.7529 - auc: 0.8279 - val_loss: 0.5421 - val_accuracy: 0.7367 - val_auc: 0.8082\n","\n","Epoch 00371: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-371epoch.h5\n","Epoch 372/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5178 - accuracy: 0.7536 - auc: 0.8280 - val_loss: 0.5430 - val_accuracy: 0.7364 - val_auc: 0.8075\n","\n","Epoch 00372: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-372epoch.h5\n","Epoch 373/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7534 - auc: 0.8281 - val_loss: 0.5428 - val_accuracy: 0.7366 - val_auc: 0.8077\n","\n","Epoch 00373: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-373epoch.h5\n","Epoch 374/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5418 - val_accuracy: 0.7371 - val_auc: 0.8085\n","\n","Epoch 00374: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-374epoch.h5\n","Epoch 375/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5416 - val_accuracy: 0.7373 - val_auc: 0.8088\n","\n","Epoch 00375: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-375epoch.h5\n","Epoch 376/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5176 - accuracy: 0.7538 - auc: 0.8281 - val_loss: 0.5415 - val_accuracy: 0.7373 - val_auc: 0.8089\n","\n","Epoch 00376: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-376epoch.h5\n","Epoch 377/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7536 - auc: 0.8281 - val_loss: 0.5421 - val_accuracy: 0.7370 - val_auc: 0.8083\n","\n","Epoch 00377: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-377epoch.h5\n","Epoch 378/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5176 - accuracy: 0.7535 - auc: 0.8282 - val_loss: 0.5421 - val_accuracy: 0.7369 - val_auc: 0.8082\n","\n","Epoch 00378: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-378epoch.h5\n","Epoch 379/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7530 - auc: 0.8281 - val_loss: 0.5427 - val_accuracy: 0.7366 - val_auc: 0.8078\n","\n","Epoch 00379: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-379epoch.h5\n","Epoch 380/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7532 - auc: 0.8280 - val_loss: 0.5414 - val_accuracy: 0.7373 - val_auc: 0.8089\n","\n","Epoch 00380: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-380epoch.h5\n","Epoch 381/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7537 - auc: 0.8281 - val_loss: 0.5410 - val_accuracy: 0.7374 - val_auc: 0.8092\n","\n","Epoch 00381: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-381epoch.h5\n","Epoch 382/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7532 - auc: 0.8281 - val_loss: 0.5428 - val_accuracy: 0.7363 - val_auc: 0.8076\n","\n","Epoch 00382: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-382epoch.h5\n","Epoch 383/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7531 - auc: 0.8281 - val_loss: 0.5416 - val_accuracy: 0.7373 - val_auc: 0.8088\n","\n","Epoch 00383: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-383epoch.h5\n","Epoch 384/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7536 - auc: 0.8281 - val_loss: 0.5418 - val_accuracy: 0.7371 - val_auc: 0.8086\n","\n","Epoch 00384: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-384epoch.h5\n","Epoch 385/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5179 - accuracy: 0.7534 - auc: 0.8279 - val_loss: 0.5416 - val_accuracy: 0.7373 - val_auc: 0.8086\n","\n","Epoch 00385: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-385epoch.h5\n","Epoch 386/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7536 - auc: 0.8280 - val_loss: 0.5420 - val_accuracy: 0.7367 - val_auc: 0.8082\n","\n","Epoch 00386: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-386epoch.h5\n","Epoch 387/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5178 - accuracy: 0.7540 - auc: 0.8280 - val_loss: 0.5438 - val_accuracy: 0.7353 - val_auc: 0.8067\n","\n","Epoch 00387: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-387epoch.h5\n","Epoch 388/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5179 - accuracy: 0.7533 - auc: 0.8279 - val_loss: 0.5416 - val_accuracy: 0.7374 - val_auc: 0.8087\n","\n","Epoch 00388: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-388epoch.h5\n","Epoch 389/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5176 - accuracy: 0.7533 - auc: 0.8282 - val_loss: 0.5433 - val_accuracy: 0.7359 - val_auc: 0.8072\n","\n","Epoch 00389: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-389epoch.h5\n","Epoch 390/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5176 - accuracy: 0.7534 - auc: 0.8281 - val_loss: 0.5420 - val_accuracy: 0.7370 - val_auc: 0.8085\n","\n","Epoch 00390: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-390epoch.h5\n","Epoch 391/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5177 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5411 - val_accuracy: 0.7374 - val_auc: 0.8093\n","\n","Epoch 00391: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-391epoch.h5\n","Epoch 392/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5177 - accuracy: 0.7535 - auc: 0.8281 - val_loss: 0.5422 - val_accuracy: 0.7367 - val_auc: 0.8082\n","\n","Epoch 00392: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-392epoch.h5\n","Epoch 393/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7532 - auc: 0.8279 - val_loss: 0.5418 - val_accuracy: 0.7369 - val_auc: 0.8085\n","\n","Epoch 00393: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-393epoch.h5\n","Epoch 394/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5426 - val_accuracy: 0.7365 - val_auc: 0.8077\n","\n","Epoch 00394: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-394epoch.h5\n","Epoch 395/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5177 - accuracy: 0.7533 - auc: 0.8281 - val_loss: 0.5415 - val_accuracy: 0.7373 - val_auc: 0.8088\n","\n","Epoch 00395: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-395epoch.h5\n","Epoch 396/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5177 - accuracy: 0.7532 - auc: 0.8281 - val_loss: 0.5422 - val_accuracy: 0.7368 - val_auc: 0.8081\n","\n","Epoch 00396: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-396epoch.h5\n","Epoch 397/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 15ms/step - loss: 0.5178 - accuracy: 0.7533 - auc: 0.8280 - val_loss: 0.5420 - val_accuracy: 0.7368 - val_auc: 0.8084\n","\n","Epoch 00397: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-397epoch.h5\n","Epoch 398/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5178 - accuracy: 0.7534 - auc: 0.8280 - val_loss: 0.5425 - val_accuracy: 0.7370 - val_auc: 0.8079\n","\n","Epoch 00398: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-398epoch.h5\n","Epoch 399/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 45s 14ms/step - loss: 0.5176 - accuracy: 0.7536 - auc: 0.8281 - val_loss: 0.5413 - val_accuracy: 0.7372 - val_auc: 0.8090\n","\n","Epoch 00399: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-399epoch.h5\n","Epoch 400/400\n","Learning rate:  5e-07\n","3113/3113 [==============================] - 43s 14ms/step - loss: 0.5178 - accuracy: 0.7535 - auc: 0.8280 - val_loss: 0.5418 - val_accuracy: 0.7370 - val_auc: 0.8085\n","\n","Epoch 00400: saving model to /content/drive/MyDrive/Projects/GSoC 2021/Model/ResNet-v1-400epoch.h5\n","Elapsed time: 2:20:21.99\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SpndV2K_mp9g"},"source":["The trained neural network can now be evaluated."]},{"cell_type":"code","metadata":{"id":"Rcllz7JOmp9h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623270798503,"user_tz":-420,"elapsed":21048,"user":{"displayName":"Hiruka Shiro","photoUrl":"","userId":"01239836994690731741"}},"outputId":"cdf7ba56-16f3-4680-c639-4dcb04324907"},"source":["# Score trained model.\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","print('Test loss:', scores[0])\n","print('Test accuracy:', scores[1])"],"execution_count":26,"outputs":[{"output_type":"stream","text":["3113/3113 [==============================] - 12s 4ms/step - loss: 0.5428 - accuracy: 0.7360 - auc: 0.8076\n","Test loss: 0.5427847504615784\n","Test accuracy: 0.7360240817070007\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0kn-FYT9pqpY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjhUkD1C1JuH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wEODjDcIuR8N"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLyp7bQhnaXj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbCQrBwK3VrY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vO0OsbO0Szks"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2YGnI23ZrB5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hBvH9IQsgiuQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8x9gL2g-NIa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSz21c4spqsE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oeHrv8PApquu"},"source":[""],"execution_count":null,"outputs":[]}]}
